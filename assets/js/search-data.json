{
  
    
        "post0": {
            "title": "The Pandas Reference",
            "content": "About . Much of data exists in rectangular format with rows and columns. Different terms can be used to describe these kind of data . Table | Data frame | Structured data | Spreadsheets | Pandas is one of the widely used data manipulation library in python for structured datasets. Below is a summary of the key operations essential for performing data analysis project(SQL equivalents). . Select column references | Select scalar expression | Where | Group By | Select aggregation | Order By | Window functions | When I started using pandas, realized that there are multiple ways to perform the same operations.Also, code I was writing was not as elegant as SQL queries and hard to debug. In this blog post I will share examples of how to perform the above mentioned SQL operations in pandas and write pandas code that is readable and easy to maintain. . import pandas as pd df = pd.read_csv(&quot;https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv&quot;) pd.options.display.max_rows = 20 . df.head(5) . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . Select columns . Use loc with list of columns names to perform selection of columns . .loc[:,[&#39;col1&#39;,&#39;col2&#39;]] . Select total_bill and tips column from the data. Note: we are using chaining to perform operations one after another . (df .loc[:,[&#39;tip&#39;,&#39;sex&#39;]] .head() ) . tip sex . 0 1.01 | Female | . 1 1.66 | Male | . 2 3.50 | Male | . 3 3.31 | Male | . 4 3.61 | Female | . Select columns manipulation . Use assign statement to add new columns, updated existing columns . .assign(new_col=1) .assign(new_col=lambda x:x[&#39;col&#39;]+1) . (df .loc[:,[&#39;total_bill&#39;,&#39;tip&#39;,&#39;sex&#39;,&#39;day&#39;,&#39;time&#39;]] .assign(percentage_tip=lambda x:x[&#39;tip&#39;]/x[&#39;total_bill&#39;]) .head() ) . total_bill tip sex day time percentage_tip . 0 16.99 | 1.01 | Female | Sun | Dinner | 0.059447 | . 1 10.34 | 1.66 | Male | Sun | Dinner | 0.160542 | . 2 21.01 | 3.50 | Male | Sun | Dinner | 0.166587 | . 3 23.68 | 3.31 | Male | Sun | Dinner | 0.139780 | . 4 24.59 | 3.61 | Female | Sun | Dinner | 0.146808 | . Filter rows (where) . Use query to peform filting of rows in pandas . .query(&quot;col1&gt;=&#39;10&#39;&quot;) . #filter only transaction with more than 15% in tips (df .loc[:,[&#39;total_bill&#39;,&#39;tip&#39;,&#39;sex&#39;,&#39;day&#39;,&#39;time&#39;]] .assign(percentage_tip=lambda x:x[&#39;tip&#39;]/x[&#39;total_bill&#39;]) .query(&quot;percentage_tip&gt;.15&quot;) .head() ) . total_bill tip sex day time percentage_tip . 1 10.34 | 1.66 | Male | Sun | Dinner | 0.160542 | . 2 21.01 | 3.50 | Male | Sun | Dinner | 0.166587 | . 5 25.29 | 4.71 | Male | Sun | Dinner | 0.186240 | . 6 8.77 | 2.00 | Male | Sun | Dinner | 0.228050 | . 9 14.78 | 3.23 | Male | Sun | Dinner | 0.218539 | . per_tip=.15 #using @ within query to refer a variable in the filter (df .loc[:,[&#39;total_bill&#39;,&#39;tip&#39;,&#39;sex&#39;,&#39;day&#39;,&#39;time&#39;]] .assign(percentage_tip=lambda x:x[&#39;tip&#39;]/x[&#39;total_bill&#39;]) .query(&quot;percentage_tip&gt;@per_tip&quot;) .head() ) . total_bill tip sex day time percentage_tip . 1 10.34 | 1.66 | Male | Sun | Dinner | 0.160542 | . 2 21.01 | 3.50 | Male | Sun | Dinner | 0.166587 | . 5 25.29 | 4.71 | Male | Sun | Dinner | 0.186240 | . 6 8.77 | 2.00 | Male | Sun | Dinner | 0.228050 | . 9 14.78 | 3.23 | Male | Sun | Dinner | 0.218539 | . Group By and Aggregation . Use groupby with named aggs to perform any type of aggregations . #By day get average and total bill (df .groupby([&#39;day&#39;]) .agg(avg_bill=(&#39;total_bill&#39;,&#39;mean&#39;) ,total_bill=(&#39;total_bill&#39;,&#39;sum&#39;)) .reset_index() ) . day avg_bill total_bill . 0 Fri | 17.151579 | 325.88 | . 1 Sat | 20.441379 | 1778.40 | . 2 Sun | 21.410000 | 1627.16 | . 3 Thur | 17.682742 | 1096.33 | . Ordering rows . Use assign statement to add new columns, updated existing columns . .sort_values([&#39;col1&#39;,&#39;col2&#39;],ascending=[True,False]) . #By day get average and total bill.Sort the output by total_bill (df .groupby([&#39;day&#39;]) .agg(avg_bill=(&#39;total_bill&#39;,&#39;mean&#39;) ,total_bill=(&#39;total_bill&#39;,&#39;sum&#39;)) .reset_index() .sort_values([&#39;total_bill&#39;]) ) . day avg_bill total_bill . 0 Fri | 17.151579 | 325.88 | . 3 Thur | 17.682742 | 1096.33 | . 2 Sun | 21.410000 | 1627.16 | . 1 Sat | 20.441379 | 1778.40 | . Conclusion . In this blog post shared some simple tips helped to improve efficiency of my data analysis projects. I plan to update this blog post with more examples to make data analysis in pandas easy. .",
            "url": "https://anaveenan.github.io/naveenanarjunan/pandas/python/2020/03/05/the-pandas-reference.html",
            "relUrl": "/pandas/python/2020/03/05/the-pandas-reference.html",
            "date": " • Mar 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Confidence Interval Plot Python",
            "content": "About . This blog post details how to create confidence interval plot in python using Altair Visualization package. Altair is a declarative statistical visualization library based on vega and vega-lite. This is one my favourite visualization package in pythons. More details can be found here . Lets load the package and get data from cars data set. . import altair as alt import numpy as np import pandas as pd from vega_datasets import data source = data.cars() source.head() . Name Miles_per_Gallon Cylinders Displacement Horsepower Weight_in_lbs Acceleration Year Origin . 0 chevrolet chevelle malibu | 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 1970-01-01 | USA | . 1 buick skylark 320 | 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 1970-01-01 | USA | . 2 plymouth satellite | 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 1970-01-01 | USA | . 3 amc rebel sst | 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 1970-01-01 | USA | . 4 ford torino | 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 1970-01-01 | USA | . Create a plot showing how mile per gallon change by year . Altair has built in capabilities to create this visualization . Lets create a base line chart showing the average mile per gallon per year | Create a confidence interval band chart using the mark_errorband() | Layer the line and CI band chart to create the final visualization | line = (alt .Chart(source).mark_line(color=&#39;blue&#39;) .encode(x=&#39;Year&#39;, y=&#39;mean(Miles_per_Gallon)&#39;)) band = (alt .Chart(source) .mark_errorband(extent=&#39;ci&#39;,color=&#39;blue&#39;) .encode(x=&#39;Year&#39;, y=alt.Y(&#39;Miles_per_Gallon&#39;, title=&#39;Miles/Gallon&#39;))) (band + line).properties(title=&#39;Confidence Interval Plot of miles per gallon&#39;) . Lets say if you want to understand how mileage varies by origin. This can done by simply encoding color in the plot . line = (alt .Chart(source).mark_line(color=&#39;blue&#39;) .encode(x=&#39;Year&#39;, y=&#39;mean(Miles_per_Gallon)&#39;, color=&#39;Origin&#39;)) band = (alt .Chart(source) .mark_errorband(extent=&#39;ci&#39;,color=&#39;blue&#39;) .encode(x=&#39;Year&#39;, y=alt.Y(&#39;Miles_per_Gallon&#39;, title=&#39;Miles/Gallon&#39;), color=&#39;Origin&#39;)) (band + line).properties(title=&#39;Confidence Interval of miles per gallon by country&#39;) . Create confidence interval plot from grouped data . Most of situation in real world you have large a dataset and still need to plot confidence interval plots.In this scenario it is better to pre compute the confidence interval based on mean and margin of error. Lets create a pandas dataframe with required fields as show below : . df=(source .groupby([&#39;Year&#39;]) .agg(avg_mpg=(&#39;Miles_per_Gallon&#39;,&#39;mean&#39;), std_mpg=(&#39;Miles_per_Gallon&#39;,&#39;std&#39;), n=(&#39;Miles_per_Gallon&#39;,&#39;count&#39;)) .assign(ul=lambda x:x[&#39;avg_mpg&#39;]+1.96*x[&#39;std_mpg&#39;]/np.sqrt(x[&#39;n&#39;]), ll=lambda x:x[&#39;avg_mpg&#39;]-1.96*x[&#39;std_mpg&#39;]/np.sqrt(x[&#39;n&#39;])) .reset_index() ) df.head() . Year avg_mpg std_mpg n ul ll . 0 1970-01-01 | 17.689655 | 5.339231 | 29 | 19.632937 | 15.746373 | . 1 1971-01-01 | 21.250000 | 6.591942 | 28 | 23.691690 | 18.808310 | . 2 1972-01-01 | 18.714286 | 5.435529 | 28 | 20.727634 | 16.700938 | . 3 1973-01-01 | 17.100000 | 4.700245 | 40 | 18.556621 | 15.643379 | . 4 1974-01-01 | 22.703704 | 6.420010 | 27 | 25.125345 | 20.282062 | . Few lines of code below create the custom confidence interval plot required . line = (alt .Chart() .mark_line(color=&#39;blue&#39;) .encode(x=&#39;Year&#39;, y=&#39;avg_mpg&#39;)) band = (alt .Chart() .mark_area(opacity=0.5,color=&#39;blue&#39;) .encode(x=&#39;Year&#39;, y=alt.Y(&#39;ll&#39;, axis=alt.Axis(title=&#39;Miles/Gallon&#39;,ticks=False)), y2=alt.Y2(&#39;ul&#39;))) alt.layer(band + line,data=df).properties(title=&#39;Confidence Interval of miles per gallon by country(Custom)&#39;) . Conclusion . Confidence interval plot is one the most important tool in a data scientist tool kit to understand uncertainty of the metrics. Altair provides excellent visualization capabilities to make this plot few line of python code. .",
            "url": "https://anaveenan.github.io/naveenanarjunan/altair/python/2020/02/22/confidence-interval-plot.html",
            "relUrl": "/altair/python/2020/02/22/confidence-interval-plot.html",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://anaveenan.github.io/naveenanarjunan/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Using Simulation to Estimate the Power of an A/B experiment",
            "content": "About . This article was originally posted in my medium blog post here . Power of an experiment measures the ability of the experiment to detect a specific alternate hypothesis. For example, an e-commerce company is trying to increase the time users spend on the website by changing the design of the website. They plan to use the well-known two-sample t-test. Power helps in answering the question: will the t-test be able to detect a difference in mean time spend (if it exists) by rejecting the null hypothesis? . Lets state the hypothesis . Null Hypothesis H0: New design has no effect on the time users spend on the website Alternate Hypothesis Ha: New design impacts the time users spend on the website . When an A/B experiment is run to measure the impact of the website redesign, we want to ensure that the experiment has at least 80% power. The following parameters impact the power of the experiment: . 1. Sample size(n): Larger the sample size, smaller the standard error becomes; and makes sampling distribution smaller. Increasing the sample size, increases the power of the experiment 2. Effect size(𝛿): Difference between the means sampling distribution of null and alternative hypothesis. Smaller the effect size, need more samples to detect an effect at predefined power 3. Alpha(𝛼): Significance value is typically set at 0.05; this is the cut off at which we accept or reject our null hypothesis. Making alpha smaller requires more samples to detect an effect at predefined power 4. Beta(β): Power is defined as 1-β . Why power analysis is done to determine sample size before running an experiment? . Running experiments is expensive and time consuming | Increases the chance of finding significant effect | Increases the chance of replicating an effect detected in an experiment | For example, the time users spend currently on the website is normally distributed with mean 2 minutes and standard deviation 1 minute. The product manager wants to design an experiment to understand if the redesigned website helps in increasing the time spent on the website. . The experiment should be able to detect a minimum of 5% change in time spent on the website. For a test like this, an exact solution is available to estimate sample size since sampling distribution is known. Here we will use the simulation method to estimate the sample and validate the same using exact method. . The following steps estimate the power of two-sample t-test: . Simulate data for the model under null 𝒩(2,1) and alternate hypothesis 𝒩(2+𝛿,1) | Perform t-test on the sample and record whether the t-test rejects the null hypothesis | Run the simulation multiple number of times and count the number of times the t-test rejects the null hypothesis. | Code to compute power of experiment for a specified sample size, effect size and significance level: . Power of the experiment is 58.8% with sample size of 1000 . import numpy as np import scipy.stats as st # Initialize delta(minimum lift the product manager expect), control_mean, control_sd delta=0.05 control_mean=2 control_sd=1 sample_size=1000 alpha=0.05#significance of the experiment n_sim=1000#Total number of samples to simulate np.random.seed(123)#set seed def simulate_data(control_mean,control_sd,sample_size,n_sim): # Simulate the time spend under null hypothesis control_time_spent = np.random.normal(loc=control_mean, scale=control_sd, size=(sample_size,n_sim)) # Simulate the time spend under alternate hypothesis treatment_time_spent = np.random.normal(loc=control_mean*(1+delta), scale=control_sd, size=(sample_size,n_sim)) return control_time_spent,treatment_time_spent # Run the t-test and get the p_value control_time_spent, treatment_time_spent=simulate_data(control_mean,control_sd,sample_size,n_sim) t_stat, p_value = st.ttest_ind(control_time_spent, treatment_time_spent) power=(p_value&lt;0.05).sum()/n_sim print(&quot;Power of the experiment {:.1%}&quot;.format(power)) #Power of the experiment 58.8% . Power of the experiment 58.8% . Code to compute sample size required to reach 80% power for specified effect size and significance level: . Based on simulation methods we need 1560 users to reach power of 80% and this closely matches with sample size estimated using exact method . #increment sample size till required power is reached sample_size=1000 np.random.seed(123) while True: control_time_spent, treatment_time_spent=simulate_data(control_mean,control_sd,sample_size,n_sim) t_stat, p_value = st.ttest_ind(control_time_spent, treatment_time_spent) power=(p_value&lt;alpha).sum()/n_sim if power&gt;.80: print(&quot;Minimum sample size required to reach significance {}&quot;.format(sample_size)) break else: sample_size+=10 #Minimum sample size required to reach significance 1560 . Minimum sample size required to reach significance 1560 . Code to compute sample size using exact method: . #Analtyical solution to compute sample size from statsmodels.stats.power import tt_ind_solve_power treat_mean=control_mean*(1+delta) mean_diff=treat_mean-control_mean cohen_d=mean_diff/np.sqrt((control_sd**2+control_sd**2)/2) n = tt_ind_solve_power(effect_size=cohen_d, alpha=alpha, power=0.8, ratio=1, alternative=&#39;two-sided&#39;) print(&#39;Minimum sample size required to reach significance: {:.0f}&#39;.format(round(n))) . Minimum sample size required to reach significance: 1571 . Conclusion . This article explained how simulation can be used to estimate power of an A/B experiment when a closed form solution doesn’t exist. .",
            "url": "https://anaveenan.github.io/naveenanarjunan/a/b%20testing/python/2020/02/14/power-analyis.html",
            "relUrl": "/a/b%20testing/python/2020/02/14/power-analyis.html",
            "date": " • Feb 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Audience Splitting in A/B Experiments",
            "content": "About . One key element in running a A/B experiment is splitting of audience based on the unit of diversion. Most of the experiment platforms does the splitting of audience for us. But there are situation in which analyst need to run an A/B experiment and splitting of audience need to performed by the analyst. In most of the organizations data is stored in a database and it would be nice if we can perform treatment assignment in SQL . Also, we need the audience split to perform post-hoc analysis of the experiment. In this blog, I will show how to perform audience splitting in spark and Hive using an example. . Data Preparation . Lets create a spark session in local. | Lets create a dummy dataset with 100,000 customers along with gender information. | Add uuid column to the dataframe to uniquely identify a user. | Convert pandas dataframe to a spark dataframe | Register the spark dataframe as &quot;user_table&quot; to be accessed in Hive | import pyspark import altair as alt import numpy as np import pandas as pd import uuid import scipy.stats as sc from vega_datasets import data from pyspark.sql import SparkSession spark = SparkSession .builder .enableHiveSupport() .getOrCreate() customers = (pd.DataFrame({&#39;user&#39;: np.arange(100000), &#39;gender&#39;:[np.random.choice([&#39;m&#39;,&#39;f&#39;], p=[0.55,0.45]) for _ in np.arange(100000)]}) .assign(user_uuid=[uuid.uuid4() for _ in range(100000)]) ) customers.head() . user gender user_uuid . 0 0 | m | 817be0d1-067c-41b8-86bc-ef6ab335ff46 | . 1 1 | m | afbac2c3-c2ae-413d-9d00-712da8ce5eb2 | . 2 2 | m | c8c990fa-7884-4c1d-89e2-d5e8af0a33fe | . 3 3 | m | 43fd874f-4644-405a-ae5e-44c01c7d3871 | . 4 4 | f | 9d78651b-d55f-4d7b-bce7-5d036b95ac6c | . sdf=spark.createDataFrame(customers.astype(str)) sdf.createOrReplaceTempView(&quot;user_table&quot;) sdf.toPandas().head() . user gender user_uuid . 0 0 | f | 12d288b0-91e3-471c-849f-38b6e3961a88 | . 1 1 | m | b1ea28f2-35fd-4334-92f9-e19fb3cfc924 | . 2 2 | f | e636cd3d-6182-4ee0-98d9-bed9350c996d | . 3 3 | f | 5f053ff3-5965-4114-808e-636e83c22647 | . 4 4 | f | f32af45d-36ff-4996-9704-99f9143a03de | . Audience splitting . Cool hashing trick to perform audience splitting . Select the unit of diversion key : user_uuid in our case (or the ID field we want to split on). | And a salt(&#39;new_widget&#39; in our example), unique value to identify our experiment. | Concatenate car_uuid with the salt selected. | Apply a hashing algorithm like md5 hash to split audience into treatment and control | query=&quot;&quot;&quot;select user_uuid, if( conv( substr( md5(concat(user_uuid, &#39;-&#39;,&#39;new_widget&#39;)), 1, 6), 16,10)/conv(&#39;ffffff&#39;,16,10) &gt; 0.50, &#39;treatment&#39;, &#39;control&#39;) as treatment ,gender from user_table &quot;&quot;&quot; df_audience=spark.sql(query).toPandas() . Validation of assignment . Chi-Square test of indepence is our friend . Lets visualize the split and looks like assignment is 50-50. But how do we validate this with statistically rigor ? . (df_audience .groupby(&#39;treatment&#39;) .agg(users=(&#39;user_uuid&#39;,&#39;count&#39;)) .reset_index() .assign(percent_users=lambda x:(x[&#39;users&#39;]/x[&#39;users&#39;].sum())*100) .style.format({&#39;percent_users&#39;:&#39;{0:.2f}%&#39;.format}) ) . treatment users percent_users . 0 control | 50180 | 50.18% | . 1 treatment | 49820 | 49.82% | . One way to validate this is see if distribution of gender is random across treatment and control. This can be translated in to a chi square test with the following hypothesis: . Null Hypothesis H0: Gender is independent of treatment assignment Alternate Hypothesis Ha: Gender is not independent of treatment assignment . Let&#39;s run an chi-square test. P-value of 0.14 indicates we can&#39;t reject the null hypothesis - gender is independent of the treatment assignment . chi2, p, dof, expected=sc.chi2_contingency(pd.crosstab(df_audience.treatment, df_audience.gender, values=df_audience.user_uuid, aggfunc=&#39;count&#39;)) print (&quot;p-value is {}&quot;.format(p)) . p-value is 0.14426225571462634 . Conclusion . Hashing is very useful technique to assign users to treatment and control in a deterministic way. Using the user_uuid and salt we can get the experiment assignment back. This can also be done easily in any SQL database .",
            "url": "https://anaveenan.github.io/naveenanarjunan/altair/python/2020/01/15/audience-hashing.html",
            "relUrl": "/altair/python/2020/01/15/audience-hashing.html",
            "date": " • Jan 15, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://anaveenan.github.io/naveenanarjunan/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://anaveenan.github.io/naveenanarjunan/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://anaveenan.github.io/naveenanarjunan/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}